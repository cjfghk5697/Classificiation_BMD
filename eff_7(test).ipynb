{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cjfghk5697/Classificiation_BMD/blob/main/eff_7(test).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uBKmHL7V3Lz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e54d6aaa-e692-4844-afb9-c29590f63466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 34.7 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 327 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 376 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 50.1 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 50.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 58.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 63.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 65.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 68.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 67.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 68.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 69.9 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 70.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: torchmetrics, timm\n",
            "Successfully installed timm-0.4.12 torchmetrics-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install timm torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cIE6v9LWWdUQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchmetrics import AUROC, ROC\n",
        "from pathlib import Path\n",
        "\n",
        "import gdown\n",
        "global PATH\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "import random\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from google.colab import files\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torchsummary import summary\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "import pandas as pd\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from glob import glob\n",
        "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "import timm\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9ecmykxxoKD",
        "outputId": "4427e577-4370-4cbb-9b15-fae8eb6e4794"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aEYiPGfk40gg"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/MyDrive/bmd_dataset/train'\n",
        "model_root = './model/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAdam(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= eps:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
        "        if not 0.0 <= betas[0] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
        "        if not 0.0 <= betas[1] < 1.0:\n",
        "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
        "        \n",
        "        self.degenerated_to_sgd = degenerated_to_sgd\n",
        "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
        "            for param in params:\n",
        "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
        "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
        "        super(RAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(RAdam, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "\n",
        "                state['step'] += 1\n",
        "                buffered = group['buffer'][int(state['step'] % 10)]\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "\n",
        "                    # more conservative since it's an approximated value\n",
        "                    if N_sma >= 5:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    elif self.degenerated_to_sgd:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = -1\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # more conservative since it's an approximated value\n",
        "                if N_sma >= 5:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "                elif step_size > 0:\n",
        "                    if group['weight_decay'] != 0:\n",
        "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
        "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
        "                    p.data.copy_(p_data_fp32)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "wPvnzTlYVwb3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzFhCKMKWrck"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WC1ecyVaWo36"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRIPXwPQWg2c"
      },
      "source": [
        "Model list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DVn5u8T8Wjoo"
      },
      "outputs": [],
      "source": [
        "# all_densenet_models = timm.list_models('**')\n",
        "# all_densenet_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODwW-HhKWicu"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wvp2Qs3aWkS_"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('efficientnet_b7', pretrained=False)\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "        self.fc = nn.Linear(1000, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.backbone(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru7jRUSVWogO"
      },
      "source": [
        "Making a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tcY0SXkdWsZP"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "dataset = ImageFolder(train_dir, transform=train_transform)\n",
        "\n",
        "# normal: 0, osteopenia:1, osteoporosis:2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHb_pzO93L0X"
      },
      "source": [
        "Train & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UhmqC0m1WxzH"
      },
      "outputs": [],
      "source": [
        "net = Model()\n",
        "net.train()\n",
        "net = net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = RAdam(net.parameters(), lr=1e-3, betas=(0.9, 0.999), weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f0MXcjiH3L0Y"
      },
      "outputs": [],
      "source": [
        "def save_model(model, acc, date_time, name):\n",
        "    model_name = name + '.pth'\n",
        "    model_path = Path(model_root + date_time)\n",
        "    model_path.mkdir(parents=True, exist_ok=True)\n",
        "    print('Saving model (Accuracy {:.2f}%) to {}'.format(acc*100, str(model_path / model_name)) )\n",
        "    torch.save({'model_state_dict':model.state_dict(), 'acc':acc}, str(model_path / model_name) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uFBGgK3vW1Yy"
      },
      "outputs": [],
      "source": [
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs, auc=False, device=device):\n",
        "    tz = datetime.timezone(datetime.timedelta(hours=9)) # Timezone infomation\n",
        "    date_time = datetime.datetime.now(tz).strftime('%Y-%m-%d-%H-%M-%S')\n",
        "    \n",
        "    best_val_acc = 0\n",
        "    best_train_acc = 0\n",
        "    epoch_train_acc = 0\n",
        "    \n",
        "    for epoch in range(num_epochs + 1):\n",
        "        print('\\n------------------------')\n",
        "        print('EPOCH {}/{}'.format(epoch, num_epochs))\n",
        "        print('------------------------')\n",
        "        \n",
        "        if auc == True:\n",
        "            auc_roc_metric = AUROC(num_classes=3, average=None)\n",
        "            roc_metric = ROC(num_classes=3)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()\n",
        "            else:\n",
        "                net.eval()\n",
        "                \n",
        "            epoch_loss = 0.0\n",
        "            epoch_corrects = 0\n",
        "            \n",
        "            total_loss = 0\n",
        "            total_size = 0\n",
        "            total_corrects = 0\n",
        "            \n",
        "            # 학습 전 성능 확인\n",
        "            if (epoch == 0) and (phase == 'train'):\n",
        "                continue\n",
        "            \n",
        "            num_iteration = len(dataloaders_dict[phase])\n",
        "            \n",
        "            for idx, (inputs, labels) in enumerate(tqdm(dataloaders_dict[phase])):\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                        \n",
        "                    epoch_loss = loss.item()\n",
        "                    total_loss += epoch_loss\n",
        "\n",
        "                    epoch_corrects = torch.sum(preds == labels.data)\n",
        "                    epoch_acc = epoch_corrects.double() / inputs.size(0)\n",
        "                    total_corrects += epoch_corrects\n",
        "                    total_size += inputs.size(0)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        print('{} [{}/{}] LOSS: {:.4f} ACC: {:.4f}'.format(phase, idx+1, num_iteration, epoch_loss, epoch_acc))\n",
        "\n",
        "                    if (phase == 'val') & (auc == True):\n",
        "                        auc_roc_metric(outputs, labels)\n",
        "                        roc_metric(outputs, labels)\n",
        "            \n",
        "            epoch_loss_avg = total_loss / num_iteration\n",
        "            epoch_acc_avg = total_corrects / total_size\n",
        "            \n",
        "            if phase == 'train':\n",
        "                if best_train_acc < epoch_acc_avg:\n",
        "                    best_train_acc = epoch_acc_avg\n",
        "                print('{} LOSS: {:.4f} ACC: {:.4f} BEST ACC: {:.4f}'.format(phase, epoch_loss_avg, epoch_acc_avg, best_train_acc))\n",
        "\n",
        "            if phase == 'val':\n",
        "                if (epoch_acc_avg == best_val_acc) & (epoch_acc_avg == best_train_acc):\n",
        "                    save_model(net, epoch_acc_avg, date_time, 'best')\n",
        "                elif best_val_acc < epoch_acc_avg:\n",
        "                    best_val_acc = epoch_acc_avg\n",
        "                    print('Best Validation Accuracy: {:.4f}'.format(epoch_acc_avg))\n",
        "                    save_model(net, epoch_acc_avg, date_time, 'best')\n",
        "\n",
        "                print('{} LOSS: {:.4f} ACC: {:.4f} BEST ACC: {:.4f}\\n'.format(phase, epoch_loss_avg, epoch_acc_avg, best_val_acc))\n",
        "\n",
        "                if auc == True:\n",
        "                    for auroc, fpt, tpr, thresholds in zip(auc_roc_metric.compute(), *roc_metric.compute()):\n",
        "                        print('auc: {:5.2f}'.format(auroc* 100))\n",
        "                        size = min(len(fpt), len(tpr))\n",
        "                        plt.plot(fpt[:size].cpu(), tpr[:size].cpu())\n",
        "                        plt.show()\n",
        "                        plt.cla()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EwisHP2kW6gX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "238b6b2b-5327-4c37-86fb-5d85d8e3fa85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------\n",
            "EPOCH 0/50\n",
            "------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/29 [00:20<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-eb0e688b48b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdataloaders_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_auc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-e02dd87b32e5>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, dataloaders_dict, criterion, optimizer, num_epochs, auc, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-660f58156dd1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x2560 and 1000x3)"
          ]
        }
      ],
      "source": [
        "num_epochs=50\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 4\n",
        "view_auc = False\n",
        "\n",
        "train_size = int(0.7*len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=num_workers)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=False, num_workers=num_workers)\n",
        "\n",
        "dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n",
        "\n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs, view_auc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bmd_sample_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}